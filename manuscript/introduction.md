{line-numbers=off}

# Introduction

The unprecedented advance in digital technology during the second half of the 20th century has produced a measurement revolution that is transforming science. In the life sciences, the genomics revolution is being driven by new technologies that permit us to observe molecular entities analogous to discovering the protons, electrons and neutrons in chemistry.  Choice examples are microarrays and next generation sequencing technology.  

Scientific fields that have traditionally relied upon simple data analysis techniques have been turned on their heads by these technologies. For example,  in the past, researchers would measure the transcription levels of a single gene of interest. Today, it is possible to measure all 20,000+ genes at once.  Advances such as these have brought about a shift from hypothesis to discovery-driven research.  However, interpreting information extracted from these massive and complex datasets requires sophisticated statistical skills as one can easily be fooled by patterns arising by chance. This has greatly elevated the importance of statistics and data analysis in this field.

In this book we will cover several of the statistical concepts and data analytic skills needed to succeed in data-driven life science research. We start with relatively basic concepts and we will explore the connection between these concepts and implementation by programming in R. 

We start with one of the most important topics in statistics, and in the life sciences: statistical inference. Inference is the use of probability to learn features of a population from data. A typical example is deciphering if two groups (for example cases versus controls) are different on average. Specific topics coveredinclude the t-test, confidence intervals, association tests, Monte Carlo methods, permutation tests and statistical power. We make use of approximations made possible by mathematical theory such as the Central Limit Theorem as well as techniques made possible by modern computing. We will learn 
how to compute p-values and confidence intervals and implement basic data analyses. Throughtout the book we will describe visualization techniques in R, useful for exploring new data sets. For example, we will use these to learn when to apply robust statistical techniques.

We then move on to an introduction to linear models and matrix algebra. We will introduce why it is beneficial to use linear models to analyze differences across groups, and why matrices are useful to represent linear models. We will review matrix algebra, including matrix notation and how to multiply matrices (both on paper and in R).
We will then apply what we covered on matrix algebra to linear models. We will learn how to fit linear models in R, how to test the significance of differences, and how the standard errors for differences are estimated. We will review some practical issues with fitting linear models, including collinearity and confounding. Finally, we will learn how to fit complex models, including interaction terms, how to contrast multiple terms in R, and the powerful technique which the functions in R actually use to robustly fit linear models: the QR decomposition.


In the final part of the book we cover more advanced topics related to high-dimensional data. We will cover statistical techniques commonly used in the analysis of high throughput data. We will discuss multiple testing, error rate controlling procedures, exploratory data analysis for high-throughput data, p-value corrections and the false discovery rate. We then cover the concepts of distance and dimension reduction.We will be learn the mathematical definition of distance. We will use this to motivate the use of the singular value decomposition (SVD) for dimension reduction and multi-dimensional scaling. Once we learn this, we will be ready to cover hierarchical and k-mean clustering. We will end with an introduction to machine learning.
We will then learn about component and factor analysis. In particular, we will examine confounding, introduce batch effects, delve into principal component analysis, factor analysis, and surrogate variable analysis. We will demonstrate how these concepts are applied in data analysis with high-throughput experimental data. We finish with by covering statistical modeling. In particular, we will discuss parametric distribution, including binomial and gamma distributions. Next, we will cover maximum likelihood estimation. Finally, we will discuss hierarchical models and empirical bayes.
